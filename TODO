How does target model updated?

How does model.fit works?

what is memory used for?

How reward works?

have multiple starting positions with different length

Principles of Depp Q learning algorithm

For what do we use reward? For training only?

What is Bellman equasion?

make first 30 steps free from punishment

make death no so punishment

make apple more rewarding.

try to understand limitation of model's vision

dont punish so much for eating pepper.

get rid of unlinear profit.

Later I will multithread learning, but not now.

16 to 12 no walls.

get rifd of going back

Implement normal path finding for reward to reward for moving in direction for food even if tailiis on the way (go around)