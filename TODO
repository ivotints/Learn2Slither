try to understand limitation of model's vision

Later I will multithread learning, but not now.

16 to 12 no walls.

get rifd of going back

Implement normal path finding for reward to reward for moving in direction for food even if tailiis on the way (go around)

bootstrapping

Why gamma in belman euasion is 0.95?

can we write a mentor/trainer for model? Or it will be too hard?

have multiple starting positions with different length

difference between soft and hard update

write all in c++

how many layers will be enough? how many neurons?


# Left direction
bit coding
Is it obstacle?
Is it good food?
Is it bad pepper?
What is the distance?

0
0
1
3

and like that for all 4 directions

# or try hybrid

# try to disable walls from equasion of state